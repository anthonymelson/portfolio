{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CartPole with DeepQ-Learning and a Sarsa Agent.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOmMHLpcIFuzZX9/sx4LgBu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anthonymelson/portfolio/blob/master/CartPole_with_DeepQ_Learning_and_a_Sarsa_Agent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HP64H00123vR",
        "colab_type": "text"
      },
      "source": [
        "# CartPole with DeepQ-Learning and a Sarsa Agent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDqwmZ1120ls",
        "colab_type": "text"
      },
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ont5cGVXttNl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "outputId": "3eb8ba58-4d60-4896-8aba-a98c0d0bfebf"
      },
      "source": [
        "import gym\n",
        "!pip install keras.rl\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Flatten\n",
        "from keras.optimizers import Adam\n",
        "from rl.agents import SARSAAgent\n",
        "from rl.policy import BoltzmannQPolicy, EpsGreedyQPolicy, MaxBoltzmannQPolicy, MaxBoltzmannQPolicy, BoltzmannGumbelQPolicy\n",
        "import numpy as np"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing /root/.cache/pip/wheels/7d/4d/84/9254c9f2e8f51865cb0dac8e79da85330c735551d31f73c894/keras_rl-0.4.2-cp36-none-any.whl\n",
            "Requirement already satisfied: keras>=2.0.7 in /usr/local/lib/python3.6/dist-packages (from keras.rl) (2.3.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras.rl) (3.13)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras.rl) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras.rl) (1.18.5)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras.rl) (1.1.2)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras.rl) (1.4.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras.rl) (2.10.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras.rl) (1.0.8)\n",
            "Installing collected packages: keras.rl\n",
            "Successfully installed keras.rl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGN7p5vX0Ajy",
        "colab_type": "text"
      },
      "source": [
        "## The CartPole Environment\n",
        "\n",
        "CartPole is a classical control problem that has been prominant in the literature since the 1980's and comes in many variations.  There is always a pole that can freely move in two directions and through all 360 degrees, which is attached to a cart (usually on a pole) that can move in the same two directions or do nothing (right, left, neither).  \n",
        "\n",
        "In this variation, the pole starts in a position selected at random from a range of possibilities, and the goal is for the agent to make the necessary series of moves (left, right, neither) required for the pole to be balanced above the cart (not fall), while staying within a given range.  If the cart balances the pole for 200 strait moves, then the agent wins and the game is \"solved\".\n",
        "\n",
        "\n",
        "**Here is a picture of the cart and pole**:\n",
        "\n",
        "![alt text](https://miro.medium.com/max/900/1*Q9gDKBugQeYNxA6ZBei1MQ.png)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jxox3FlMtuTr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ENV_NAME = 'CartPole-v0'\n",
        "\n",
        "# Get the environment and extract the number of actions.\n",
        "env = gym.make(ENV_NAME)\n",
        "np.random.seed(38)\n",
        "env.seed(65)\n",
        "nb_actions = env.action_space.n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8O2A2VNyXoK",
        "colab_type": "text"
      },
      "source": [
        "## Create Deep Learning Model\n",
        "Here we build a very simple deep learning model using keras.  It's first layer is a flattened layer with a number of nodes equal to the observation space of the game.  The next layer is an ordinary dense layer with a rectified linear unit as an activation function.  The third layer is also a dense layer, but one that has a number of nodes equal to the number of actions available to the agent in the game, and it has a linear activation (which helps it to approximate the reward function and transcends the need for discretization of the reward space).\n",
        "\n",
        "This model is essentially our agents brain.  It maps the state/action pairs the game allows to a continous reward space.  As this model becomes more accurate, the agent will make better (more rewarding) decisions--it learns!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42d_XwBbtucz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "outputId": "4011f47d-5fda-49c3-8f73-203cbadc6a1d"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
        "model.add(Dense(64))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(nb_actions))\n",
        "model.add(Activation('linear'))\n",
        "print(model.summary())"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_2 (Flatten)          (None, 4)                 0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 64)                320       \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 2)                 130       \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 2)                 0         \n",
            "=================================================================\n",
            "Total params: 450\n",
            "Trainable params: 450\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dENY6f6xuITb",
        "colab_type": "text"
      },
      "source": [
        "## Select Exploration Policy\n",
        "\n",
        "Here we are going to stick with a classic--epsilon greedy.  This policy takes a random action some percentage of the time, and follows what it currently believes is \"best policy\" the rest of the time.  Additionally, the percentage of random behavior decreases on some kind of schedule--eventually leaving an agent that does \"it's best\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWIWuC4Ft0j_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#policy = BoltzmannQPolicy()\n",
        "policy = EpsGreedyQPolicy()\n",
        "#policy = MaxBoltzmannQPolicy()\n",
        "#policy = BoltzmannGumbelQPolicy()"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ym6Ukdb1u0Wx",
        "colab_type": "text"
      },
      "source": [
        "## Select an Agent\n",
        "\n",
        "Here we are selecting sarsa (state-action-reward-state-action), which is an on-policy learning algorithm.  This means that i\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "t updates its policy based on the result of what it actually did and not on the predicted rewards of possible actions (as Q-learning does).  It also differs from the more common Q-learning because Q-learning only considers state-action-reward-state."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Weie7cCat3tr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sarsa = SARSAAgent(model=model, nb_actions=nb_actions, nb_steps_warmup=1000, policy=policy)\n",
        "sarsa.compile(Adam(lr=1e-3), metrics=['mae'])"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHLRFSGZyKyp",
        "colab_type": "text"
      },
      "source": [
        "## Train Agent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNyrMoglwpe5",
        "colab_type": "text"
      },
      "source": [
        "Okay, now it's time to learn something!\n",
        "\n",
        "I'm setting visualize to false because Colab doesn't have support for Gym's rendering capabilities.  However, if you evaluate this in spyder or on a locally hosted Jupyter notebook, there is no problem.  It is actually a great way to understand how your agent is doing as it provides rich qualitative feedback.\n",
        "\n",
        "The parameters for sarsa.fit:\n",
        "\n",
        "**env** : The environment you are solving\n",
        "\n",
        "**nb_steps** : The number of total steps in **all** episodes\n",
        "\n",
        "**visualize** : Whether to display the rendering of the game\n",
        "\n",
        "**verbose** : How much data to generate/store about the training process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HE_HBJ27t8eT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a39a0c43-b15e-406f-f98f-343f26a706db"
      },
      "source": [
        "\n",
        "sarsa.fit(env, nb_steps=10000, visualize=False, verbose=0)\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f5e50375860>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDlZ_SrLyNwq",
        "colab_type": "text"
      },
      "source": [
        "## Save Weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Emr9XK5NyBB0",
        "colab_type": "text"
      },
      "source": [
        "After training is done, we save the final weights."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MuJXZGqt9Gs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sarsa.save_weights('sarsa_EpsGreedyQ_{}_weights.h5f'.format(ENV_NAME), overwrite=True)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3549ch77ySTR",
        "colab_type": "text"
      },
      "source": [
        "## Test Agent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAERv53-yELL",
        "colab_type": "text"
      },
      "source": [
        "Finally, we evaluate our algorithm for 5 episodes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atmHGt_atq8P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "291d7c00-0d5b-435e-f44f-b6aac7de5b07"
      },
      "source": [
        "sarsa.test(env, nb_episodes=5, visualize=False)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing for 5 episodes ...\n",
            "Episode 1: reward: 41.000, steps: 41\n",
            "Episode 2: reward: 37.000, steps: 37\n",
            "Episode 3: reward: 43.000, steps: 43\n",
            "Episode 4: reward: 42.000, steps: 42\n",
            "Episode 5: reward: 37.000, steps: 37\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f5e50375908>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    }
  ]
}