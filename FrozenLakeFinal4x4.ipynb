{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FrozenLakeFinal4x4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNYYoJijr6xUz8lF8YVfV5X",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anthonymelson/portfolio/blob/master/FrozenLakeFinal4x4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgAKpt84XWHi",
        "colab_type": "text"
      },
      "source": [
        "# Reinforcement Learning in Python: Solving Frozen Lake 4x4 w Q-Learning\n",
        "\n",
        "Frozen Lake is a classic problem in reinforcement learning.  It is considered solved if the agent is able to win 78% of the time or more in 100+ trials.  Traditionally, this problem is solved with value iteration.  Lets see how Q-learning fairs!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KHLxmyUC70CG",
        "colab_type": "text"
      },
      "source": [
        "### Import Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jv2DBedBUP6C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install gym[all]\n",
        "!pip install box2d box2d-kengz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7P83ir2UVxm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrEm-Gvp758b",
        "colab_type": "text"
      },
      "source": [
        "### FrozenLake-v0 (4x4)\n",
        "\n",
        "FrozenLake-v0 is a simulated environment with 16 states.  The objective is to go from a set starting point, to a finishing point (at the top left and bottom right of a square grid) without falling into \"holes\" (of which there are 4).\n",
        "\n",
        "Though there is an obvious solution to the deterministic version where you always go where you want, the stochastic version where you only have a 1/3 chance of going to the cell you select (the rest of which being distributed among the other 3 directions and wieghted to the two closest) is much more difficult.\n",
        "\n",
        "Below is a map of the environment as it is designed in OpenAI Gym.\n",
        "\n",
        "![map](https://twice22.github.io/images/rl_series/frozenlake.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0DiFDWjz-4_y",
        "colab_type": "text"
      },
      "source": [
        "### Create the Environment\n",
        "\n",
        "Gym offers many standardized environments in many different categories.  This example is from the 'toy-text' class.  It is a classical problem and is perfect to get started with."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XqBLUhC1T2CO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "518dd65b-7af9-4d4a-dad4-40805e672680"
      },
      "source": [
        "env = gym.make('FrozenLake-v0')\n",
        "env.reset()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Vk0iqU-_SrE",
        "colab_type": "text"
      },
      "source": [
        "### Examine the Environment\n",
        "\n",
        "Gym's environment class has a small number of methods.  They are:\n",
        "\n",
        "**step()** : which takes an action as an input, and returns new state, reward, done (telling if the game is over), and info (a dict for debugging)\n",
        "\n",
        "**observation_space()** : which returns the shape and type (continuous, discrete, Box) of the observation space (states) and .n which gives the number of states\n",
        "\n",
        "**action_space()** : which returns the shape of the action space (choices) and .n which gives the number of actions\n",
        "\n",
        "**render()** : which takes 'human', 'rgb', and 'ascii' as inputs, and returns either a video, text, or dict as output that shows how the game-world is functioning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDTbKgzCtxXt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "outputId": "456aac0a-4188-4407-d9ed-75bb800a85fd"
      },
      "source": [
        "print(f\"Observation Space: {env.observation_space}\")\n",
        "print(f\"Number of States: {env.observation_space.n}\")\n",
        "print(f\"Action Space: {env.action_space}\")\n",
        "print(f\"Number of Actions: {env.action_space.n}\")\n",
        "print()\n",
        "print(\"View of Text-Map:\")\n",
        "env.render()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Observation Space: Discrete(16)\n",
            "Number of States: 16\n",
            "Action Space: Discrete(4)\n",
            "Number of Actions: 4\n",
            "\n",
            "View of Text-Map:\n",
            "\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOvGcnCBBVuk",
        "colab_type": "text"
      },
      "source": [
        "### Define Necessary Function\n",
        "We will create two functions:\n",
        "\n",
        "**act**: This will select actions from the action space.  It will mix its selections between random actions and actions based on the best policy known at the time of the decision.  As the agent plays through more episodes, the probability of random action will decrease (an exploration strategy called \"epsilon decay\").\n",
        "\n",
        "**learn**: This will apply Q-learning.  Meaning, it will update a Q-table that maps every possible state/action pair to a reward.  By updating it based on rewards from previous actions, the table will become ever better at approximating the actual value of specific actions.  In doing so, the table will converge on the \"optimal policy\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YuQTKX6xEd0R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def act(state):\n",
        "  if np.random.uniform(0,1) < epsilon:\n",
        "    action = env.action_space.sample()  \n",
        "  else:\n",
        "    action = np.argmax(q_table[state,:]) \n",
        "  return action"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GM2g3we5E5e0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def learn(state, new_state, reward, action):\n",
        "\n",
        "  q_table[state, action] = q_table[state, action] + learning_rate * (reward + gamma * np.max(q_table[new_state,:]) - q_table[state, action])"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OhO_wnJ8Cb62",
        "colab_type": "text"
      },
      "source": [
        "### Initialize Global Values\n",
        "\n",
        "**gamma** = discount rate (how much less weight to give to future rewards)\n",
        "\n",
        "**learning_rate** = amount of affect new rewards have on old mappings during each update round\n",
        "\n",
        "**episode** = the number of times the game will be started over again during agent training\n",
        "\n",
        "**epsilon** = the probability that a random action will be taken in each step\n",
        "\n",
        "**max_steps** = number of steps the agent takes in each episode\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1CuuTUHyTwpD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gamma = 0.95\n",
        "learning_rate = 0.1\n",
        "episodes = 15000\n",
        "epsilon = 1.0\n",
        "max_steps = 99"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFJftPj6DVAB",
        "colab_type": "text"
      },
      "source": [
        "### Manage Epsilon Decay\n",
        "\n",
        "The decay rate is proportional to the number of episodes so it will reach is minimum (0.1) towards the end of training.  It gets slightly smaller in each round and thus allows for less random actions to be taken.\n",
        "\n",
        "The purpose is to manage the explore/exploit trade-off, which is essential in RL.  The randomness in the begining allows the agent to explore and learn about its environment so it can evaluate its many options before it gets set on one.  \n",
        "\n",
        "As it decreases, the agent can begin doing what it has a pretty good idea works, and focus its exploration on refining those core strategies.\n",
        "\n",
        "Finally, when it runs out, it can exploit what it knows to maximize its gains."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4X6FUy9ui9I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "start_epsilon_decaying = 1\n",
        "end_epsilon_decaying = episodes // 2\n",
        "epsilon_decay_value = epsilon/(end_epsilon_decaying - start_epsilon_decaying)\n",
        "epsilon_min = 0.01"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qG9LRyhMEWMU",
        "colab_type": "text"
      },
      "source": [
        "### Create Q-Table\n",
        "\n",
        "The Q-table maps each state to it possible actions and learns the optimal mappings if updated properly.  In the case of FrozenLake-v0, there are 4 discrete options (up, down, right, left) and 16 states (4x4 square), thus the Q-table will be 4x16 or 64 spaces.\n",
        "\n",
        "This can be created by getting the number of obs and actions, and turning them into an n x m vector."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6UA6tfhT4x0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "q_table = np.zeros([env.observation_space.n, env.action_space.n])"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvxZkvzKFEFO",
        "colab_type": "text"
      },
      "source": [
        "### Run the Q-Learning Algorithm\n",
        "\n",
        "This is where the Q-table is optimized by the rewards gained from the agents exploration/exploitation of the environment.  The agent plays the game, and the table is updated based on the rewards that result from specific actions in specific states."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIWzxYXDtmjk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epsilon = 0.99\n",
        "e = 0\n",
        "terminal_state = []\n",
        "for i in range(episodes):\n",
        "  state = env.reset()\n",
        "  t = 0\n",
        "\n",
        "  while t < max_steps:\n",
        "\n",
        "      action = act(state) # Select Action\n",
        "      new_state, reward, done, info = env.step(action) # Take Action\n",
        "      reward = reward\n",
        "      learn(state, new_state, reward, action) # Get New_State and Reward\n",
        "\n",
        "      state = new_state # Update State\n",
        "      t = t + 1\n",
        "\n",
        "      if done == True:\n",
        "        break\n",
        "  e = e + 1\n",
        "  terminal_state.append(state)\n",
        " \n",
        "  if end_epsilon_decaying >= e >= start_epsilon_decaying:\n",
        "    if epsilon > epsilon_min:\n",
        "      epsilon -= epsilon_decay_value\n",
        "env.close()"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7Nu72uzFhSs",
        "colab_type": "text"
      },
      "source": [
        "### View Q-Table"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MpuAgLFE0zp6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "6df83b04-72c8-43a1-b37a-b5d5c087ec19"
      },
      "source": [
        "print(q_table)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.17596152 0.15464652 0.15368711 0.15297813]\n",
            " [0.07931432 0.09119606 0.09582933 0.14541445]\n",
            " [0.13098817 0.09723732 0.08964217 0.0898448 ]\n",
            " [0.06879116 0.06872574 0.06424997 0.09633005]\n",
            " [0.20624316 0.13098031 0.12443471 0.12513763]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.10841471 0.0437884  0.09200703 0.0458038 ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.19012222 0.1815462  0.20665824 0.27173   ]\n",
            " [0.15328256 0.35346571 0.28020713 0.26414851]\n",
            " [0.34376092 0.23803768 0.22547474 0.16984093]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.21585321 0.28962942 0.49030649 0.27073966]\n",
            " [0.53705548 0.84664011 0.56509102 0.45630285]\n",
            " [0.         0.         0.         0.        ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OeLCXYt0X9rb",
        "colab_type": "text"
      },
      "source": [
        "### Test Learner\n",
        "\n",
        "Runs for 100 episodes with the trained agent and counts how many times the it wins.  It wins 82 times in this round of 100, which is enough to \"pass\" (according to the original problem definition for Frozen Lake 4x4 which is 78% over 100 trials).\n",
        "\n",
        "It prints the terminal frame of each game (18 ending in holes, and 82 ending at the goal)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7Gu3nTJVakm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        },
        "outputId": "61980c8c-5c02-497c-e416-45a8aba70d17"
      },
      "source": [
        "win = []\n",
        "for i in range(100):\n",
        "  state = env.reset()\n",
        "  moves = 0\n",
        "\n",
        "  for _ in range(300):\n",
        "    action = np.argmax(q_table[state])\n",
        "    state_new, reward, done, _ = env.step(action)\n",
        "\n",
        "    #env.render()\n",
        "    state = state_new\n",
        "    moves = moves + 1\n",
        "\n",
        "    if done:\n",
        "      break\n",
        "  win.append(reward)\n",
        "  \n",
        "  # Render last 5 games\n",
        "  if i >= 95:\n",
        "    env.render()\n",
        "  #print(moves)\n",
        "print(sum(win))"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "  (Left)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "82.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGzS5fbJWEmO",
        "colab_type": "text"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "With Q-learning we were able to solve the stochastic frozen lake 4x4 problem.  This indicates that our agent was not only able to account for the small number of paths, but also for the randomness built into the game.  This level of strategy is what AI (specifically RL) is all about."
      ]
    }
  ]
}